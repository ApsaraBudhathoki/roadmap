**Deep learning (DL)** is a machine learning subfield that uses multiple layers for learning data representations

**DL** is exceptionally effective at learning patterns.

**DL **applies a multi-layer process for learning rich hierarchical  features (i.e., data representations)

**Neural Network:**


![0_zd5njhAW1tLg_197](https://user-images.githubusercontent.com/76399951/195997457-1e3d830f-a2a1-4838-b9b5-6dec6cc8071c.png)

**Artificial Neural Network :**

![three-layered-architecute-in-artificial-neural-net](https://user-images.githubusercontent.com/76399951/195997567-464c30c4-f39b-42c7-8960-642afb9d93a7.png)

Artificial Neural Networks (ANN) are algorithms based on brain function and are used to model complicated patterns and forecast issues. The Artificial Neural Network (ANN) is a deep learning method that arose from the concept of the human brain Biological Neural Networks. The development of ANN was the result of an attempt to replicate the workings of the human brain. The workings of ANN are extremely similar to those of biological neural networks, although they are not identical. ANN algorithm accepts only numeric and structured data.

**Convolutional neural network:**
<img width="944" alt="screen-shot-2016-08-07-at-9-15-21-pm" src="https://user-images.githubusercontent.com/76399951/195997731-3077dd94-1c12-41a1-9278-196bbeb20adf.png">

A convolutional neural network (CNN or ConvNet), is a network architecture for deep learning which learns directly from data, eliminating the need for manual feature extraction.
CNNs are particularly useful for finding patterns in images to recognize objects, faces, and scenes. They can also be quite effective for classifying non-image data such as audio, time series, and signal data.
Applications that call for object recognition and computer vision — such as self-driving vehicles and face-recognition applications — rely heavily on CNNs.

**Transfer learning:**
![transfer_learning_general](https://user-images.githubusercontent.com/76399951/195998064-f7e0955c-3cb4-4469-b5b6-8c97100dee79.png)

Transfer learning is about leveraging feature representations from a pre-trained model, so you don’t have to train a new model from scratch. 

The pre-trained models are usually trained on massive datasets that are a standard benchmark in the computer vision frontier. The weights obtained from the models can be reused in other computer vision tasks. 

These models can be used directly in making predictions on new tasks or integrated into the process of training a new model. Including the pre-trained models in a new model leads to lower training time and lower generalization error.  

Transfer learning is particularly very useful when you have a small training dataset. In this case, you can, for example, use the weights from the pre-trained models to initialize the weights of the new model.

**** Object detection:****

![0_ronLGMWhwiYuNDab](https://user-images.githubusercontent.com/76399951/195998165-130f17ad-674f-4e9c-8a97-45fc836868a5.gif)
Object detection is a computer vision technique for locating instances of objects in images or videos. Object detection algorithms typically leverage machine learning or deep learning to produce meaningful results. When humans look at images or video, we can recognize and locate objects of interest within a matter of moments. The goal of object detection is to replicate this intelligence using a computer.

**Recurrent neural network :**
![Recurrent_neural_network_unfold](https://user-images.githubusercontent.com/76399951/195998221-65115645-250e-4116-9e4e-92420a9660ef.svg)
A recurrent neural network (RNN) is a special type of an artificial neural network adapted to work for time series data or data that involves sequences. Ordinary feed forward neural networks are only meant for data points, which are independent of each other. However, if we have data in a sequence such that one data point depends upon the previous data point, we need to modify the neural network to incorporate the dependencies between these data points. RNNs have the concept of ‘memory’ that helps them store the states or information of previous inputs to generate the next output of the sequence.

**Road Map:**

![1_CQXIj_SEfhfeMpMgFE8XRg](https://user-images.githubusercontent.com/76399951/195998316-fa2f2243-2364-421f-aeb9-65abb8e41eee.jpg)

![1_NsH0YPjt6FbtZOxmGy8DTQ](https://user-images.githubusercontent.com/76399951/195998320-fe71d813-c174-4294-9320-c1c5321c0674.jpg)

